---
title: "IODS Final assignment"
author: 
- name: "Laura Matkala"
  affiliation: "laura.matkala@helsinki.fi"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    fig_height: 8
    fig_width: 10
    highlight: pygments
    toc: yes
    toc_depth: 3
---
<style>
p.caption {
  font-size: 0.9em;
}
</style>
---
# <span style="color:darkblue">Briefly</span>

In this final assignment I used dataset Boston from the R package MASS. I chose variable related to crime rate per capita was as independent variable and took a look into how it could be explained with three other variables: proportion of non-retail business acres, full-value property-tax rate and index of accessibility to radial highway. I used linear and logistic regression were, and both gave the same result that only index of accessibility to radial highway is affecting crime rate. Data was transformed for logistic regreassion and used as it is for linear regression.

# <span style="color:darkblue">Some words to start with</span>

**Welcome to the page!** This is an essential part of my final assignment to the course "Introduction to Open Data Science", or as we friends call it "IODS" course. My name is Laura Matkala and I am a PhD student who studies forests. I have to say this is one of the most inspiring courses I have taken in a while. I will do my best with all the new skills I have learned during the course to make a best possible outcome for this assignment!


```{r fig1,  fig.align="center", out.width='100%', fig.cap="Happy forest scientist by a lake in a mountain forest at Mammoth Lakes, CA, USA.(This is here to remind us that although it doesn't look like it now, the sun actually does exist...)"}
knitr::include_graphics('C:/HY-Data/MATKALA/GitHub/IODS-final/figures/49.jpg')
```



#<span style="color:darkblue">About the dataset</span>

I chose to use the dataset <span style="color:red">Boston</span>, which includes data about housing in the suburbs of Boston , Massachusettes, USA. I will later perform <span style="background:yellow">linear regression</span> and <span style="background:yellow">logistic regression</span> to the variable <span style="color:red">"crim"</span>,  but first some basic information about the dataset. 

```{r fig2, out.width='100%', fig.cap="The dataset has variables related to housing in the suburbs of Boston, Massachusettes, USA. Picture from: http://amtrakdowneaster.com/stations/boston"}
knitr::include_graphics('C:/HY-Data/MATKALA/GitHub/IODS-final/figures/boston.jpg')
```



I didn't do any data wrangling related to the part where I will perform <span style="background:yellow">linear regression</span>, but I did something for the latter <span style="background:yellow">logistic regression</span> part. You can find the R script file with all the data wrangling and codes [here](https://github.com/LauraMatkala/IODS-final/blob/master/data_wrangling_final.R). The variables in the dataset are:


* <span style="color:green">crim = per capita crime rate by town</span>
* <span style="color:purple">zn = proportion of residential land zoned for lots over 25,000 sq.ft</span>
* <span style="color:green">indus = proportion of non-retail business acres per town</span>
* <span style="color:purple">chas = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</span>
* <span style="color:green">nox = nitrogen oxides concentration (parts per 10 million)</span>
* <span style="color:purple">crm = average number of rooms per dwelling</span>
* <span style="color:green">age = proportion of owner-occupied units built prior to 1940</span>
* <span style="color:purple">dis = weighted mean of distances to five Boston employment centres</span>
* <span style="color:green">rad = index of accessibility to radial highways</span>
* <span style="color:purple">tax = full-value property-tax rate per \$10,000</span>
* <span style="color:green">ptratio = pupil-teacher ratio by town</span>
* <span style="color:purple">black = 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</span>
* <span style="color:green">lstat = lower status of the population (percent)</span>
* <span style="color:purple">medv = median value of owner-occupied homes in \$1000s</span> 
* <span style="color:green">high_crime = TRUE when crim = high, FALSE otherwise</span>



#<span style="color:darkblue">Analysis</span>


The hypotheses for both analysis are: 

**<span style="background: lightgreen">1. Index of accessibility to radial highways affects per capita crime rate.</span>**

**<span style="background: lightgreen">2. Full-value property-tax rate does not affect per capita crime rate.</span>**

**<span style="background: lightgreen">3. Proportion of non-retail business acres does not affect per capita crime rate</span>**


##<span style="color:magenta">Linear regression</span>
###<span style="color:green">Data exploration</span>


To start with the linear regression I need to read in the data as well as call the needed packages. I will also check the structure and dimensions of the data to see that everything is in order after the data wrangling and saving the file as csv.


```{r start, warning=FALSE, message=FALSE} 
Boston<-read.csv(file = "C:/HY-Data/MATKALA/GitHub/IODS-final/Boston.csv", header = TRUE, sep=",")
library(GGally); library(ggplot2)
str(Boston)
dim(Boston)
```

Everything seems to be ok with the data and it looks like I meant it to look like at this point. Let's make a couple of plots to see what the data looks like.

```{r plots, fig.height=10, fig.width=14, warning=FALSE, message=FALSE}
ggpairs(Boston, lower = list(combo = wrap("facethist", bins = 20)))

```

Ok, we see that <span style="color:red">"rad"</span>, <span style="color:red">"tax"</span> and <span style="color:red">"indus"</span> have high correlations with <span style="color:red">"crim"</span>. I will thus use those variables for creating a linear multiple regression model, where the three first mentioned variables are used as explanatory variables for <span style="color:red">"crim"</span>.  A linear multiple regression model in this case takes the form $y = \alpha+\beta_1x_1+\beta_2x_2+\beta_3x_3+\epsilon$, where rad, dis and ptratio are estimates of $\beta$, $\alpha$ is an intercept and $\epsilon$ is the error term/random noise.


###<span style="color:green">Building the model</span>


Let's start modeling!

```{r multiple regression, warning=FALSE, message=FALSE}
my_model <- lm(crim ~ rad + tax + indus, data = Boston)
summary(my_model)
```


In linear regression the aim is to minimize the residuals, which are the prediction errors of the model. The best model fit is found so that the sum of the squared residuals is minimized.  The $\beta$ values this model gives us are 0.6, 0.001 and 0.06,respectively, whereas the $\alpha$ (intercept) is -3.0. Standard error for $\alpha$ is 1.1 and for $\beta$s 0.08, 0.005 and 0.06, respectively.  Based on p-values it seems that only <span style="color:red">"rad"</span> is important for the model. I will thus leave <span style="color:red">"tax"</span> and  <span style="color:red">"indus"</span>out and formulate another model.

```{r multiple regression part two, warning=FALSE, message=FALSE}
my_model2<- lm(crim ~ rad, data = Boston)
summary(my_model2)
```

Well, no dramatic changes compared to the previous situation if we look at the residuals. The standard error for <span style="color:red">"rad"</span> as well as its p-value got smaller, though, compared to model number 1. The same happened to the intercept, so I guess in this sense the latter model is better than the first one. Anyway, I come to the same conclusion as I did when looking into this dataset in the exercise where we used LDA: with higher index of accessibility to radial highways there are better possibilities to access the highway, and thus escape after committing a crime. In other words, all my hypotheses are supported and confirmed.

##<span style="color:magenta">Logistic regression</span>
###<span style="color:green">The model and its odds ratio</span>

It's time to do another kind of analysis for the same data. I choose to use logistic regression for this. It I will use the same explanatory variables as with linear regression, except that for logistic regression I am using the dependent variable <span style="color:red">"high_crime"</span> instead of <span style="color:red">"crim"</span>, since the dependent variable in logistic regression needs to be categorical. So <span style="color:red">"high_crime"</span> is categorical, and gets the values TRUE when <span style="color:red">"crim"</span> is high and FALSE otherwise. This type of mutation is required for logistic regression because it deals with probabilities. As I mentioned earlier,   [here](https://github.com/LauraMatkala/IODS-final/blob/master/data_wrangling_final.R) you can see how I have done the mutation of the dependent variable. 

Here is the model, its summary and coefficients.

```{r logistic, warning=FALSE, message=FALSE}
m <- glm(high_crime ~ rad + tax + indus, data = Boston, family = "binomial")
summary(m)
coef(m)
```

Since the explanatory variables are not categorical, all the necessary information is shown above. According to the summary table <span style="color:red">"rad"</span> is once again the only explanatory variable that is necessary for the model. This supports hypotheses 2 and 3. Therefore, I will leave the other variables out, adjust the model to its final form, show its summary and present its <span style="background:yellow">odds ratios</span>.

```{r logistic final, warning=FALSE, message=FALSE}
m2 <- glm(high_crime ~ rad, data = Boston, family ="binomial")
summary(m2)

library(tidyr)

OR <- coef(m2) %>% exp
CI<-confint(m2) %>% exp
cbind(OR, CI)
```


If we look at the odds of <span style="color:red">"rad"</span> it is approximately 1.6. This means, if I understood correctly, that if <span style="color:red">"rad"</span> is high, there is a 1,6 times bigger chance for <span style="color:red">"high_crime"</span> to be TRUE than for it to be FALSE. But before I can say all my hypotheses are confirmed I need to look into the <span style="background:yellow">predictive power</span> of the model.

###<span style="color:green">The predictive power</span>

```{r predict,  warning=FALSE, message=FALSE}
library(dplyr)
probabilities <- predict(m2, type = "response")
Boston <- mutate(Boston, probability = probabilities)
Boston <- mutate(Boston, prediction = probability>0.5 )
select(Boston, rad, high_crime, probability, prediction) %>% tail(10)
table(high_crime = Boston$high_crime, prediction = Boston$prediction)
```

I will also calculate the training error, which gives us the total proportion of inaccurately classified individuals.

```{r training error}
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

loss_func(class = Boston$high_crime, prob = Boston$probability)


```
Well, the numbers look rather good to me. The model predicts crime rate quite well, and the training error is only 1 %. Actually, it looks suspiciously good, but maybe I will go ahead and trust this result at this point. So with certain modifications, also linear and logistic regression can be used for this dataset, and not just LDA what we used in our exercise during the course.



# <span style="color:darkblue">Finally</span>

This has been a very helpful course in both learning R and statistical methods with different types of datasets. Also, I want to thank the organizers for making this a course where learning truly is fun. This is not the case in in many courses though it should be. So thank you for being so enthusiastic about everything yourselves, that already shows good example for the students. I have already recommended this course for a couple of my friends. Keep up the good work! 
